{
 "cells": [
  {
  
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Sign Language (ASL) \n",
    "## Introduction to American Sign Language \n",
    "American Sign Language (ASL) is the primary language used by many deaf individuals in North America. It is also used by hard-of-hearing and hearing individuals. ASL is as rich as spoken languages, employing signs made with the hands, along with facial gestures and bodily postures.\n",
    "\n",
    "## Project Overview\n",
    "Recent progress has been made in developing computer vision systems that translate sign language to spoken language. These systems often rely on complex neural network architectures to detect subtle patterns in streaming video. As a first step towards understanding how to build a translation system, we can reduce the problem size by translating individual letters instead of sentences.\n",
    "\n",
    "## Project Goals\n",
    "In this notebook, we will train a convolutional neural network (CNN) to classify images of ASL letters. After loading, examining, and preprocessing the data, we will train the network and test its performance.\n",
    "\n",
    "## Data Loading\n",
    "We begin by loading the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages and set numpy random seed\n",
    "import numpy as np\n",
    "np.random.seed(5) \n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(2)\n",
    "from datasets import sign_language\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-shuffled training and test datasets\n",
    "(x_train, y_train), (x_test, y_test) = sign_language.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize the Training Data\n",
    "Next, we create a list of string-valued labels containing the letters that appear in the dataset. We visualize the first several images in the training data along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store labels of dataset\n",
    "labels = ['A', 'B', 'C']\n",
    "\n",
    "# Print the first several training images, along with the labels\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "  ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(x_train[i]))\n",
    "  ax.set_title(\"{}\".format(labels[y_train[i]]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the Dataset\n",
    "We examine how many images of each letter are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of A's in the training dataset\n",
    "num_A_train = sum(y_train == 0)\n",
    "# Number of B's in the training dataset\n",
    "num_B_train = sum(y_train == 1)\n",
    "# Number of C's in the training dataset\n",
    "num_C_train = sum(y_train == 2)\n",
    "\n",
    "# Number of A's in the test dataset\n",
    "num_A_test = sum(y_test == 0)\n",
    "# Number of B's in the test dataset\n",
    "num_B_test = sum(y_test == 1)\n",
    "# Number of C's in the test dataset\n",
    "num_C_test = sum(y_test == 2)\n",
    "\n",
    "# Print statistics about the dataset\n",
    "print(\"Training set:\")\n",
    "print(\"\\tA: {}, B: {}, C: {}\".format(num_A_train, num_B_train, num_C_train))\n",
    "print(\"Test set:\")\n",
    "print(\"\\tA: {}, B: {}, C: {}\".format(num_A_test, num_B_test, num_C_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot Encode the Data\n",
    "Keras models do not accept categorical integer labels. We need to one-hot encode the labels before supplying them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# One-hot encode the training labels\n",
    "y_train_OH = np_utils.to_categorical(y_train, 3)\n",
    "\n",
    "# One-hot encode the test labels\n",
    "y_test_OH = np_utils.to_categorical(y_test, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Model\n",
    "We define a CNN to classify the data. The network accepts an image of an ASL letter as input and returns the predicted probabilities that the image belongs in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# First convolutional layer accepts image input\n",
    "model.add(Conv2D(filters=5, kernel_size=5, padding='same', activation='relu', \n",
    "            input_shape=(50, 50, 3)))\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(filters=15, kernel_size=5, padding='same', activation='relu'))\n",
    "# Add another max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile the Model\n",
    "We compile the model by specifying the optimizer, loss function, and a metric to track during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', \n",
    "       loss='categorical_crossentropy', \n",
    "       metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "We fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hist = model.fit(x_train, y_train_OH, validation_split=0.2, epochs=2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model\n",
    "We evaluate the model on the test dataset to determine its accuracy on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain accuracy on test set\n",
    "score = model.evaluate(x_test, y_test_OH, verbose=0)\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Mistakes\n",
    "Finally, we visualize the images that were incorrectly classified by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for test dataset\n",
    "y_probs = model.predict(x_test)\n",
    "\n",
    "# Get predicted labels for test dataset\n",
    "y_preds = np.argmax(y_probs, axis=1)\n",
    "\n",
    "# Indices corresponding to test images which were mislabeled\n",
    "bad_test_idxs = np.where(y_preds != y_test)[0]\n",
    "\n",
    "# Print mislabeled examples\n",
    "fig = plt.figure(figsize=(25,4))\n",
    "for i, idx in enumerate(bad_test_idxs):\n",
    "  ax = fig.add_subplot(2, np.ceil(len(bad_test_idxs)/2), i + 1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(x_test[idx]))\n",
    "  ax.set_title(\"{} (pred: {})\".format(labels[y_test[idx]], labels[y_preds[idx]]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i've added some more files, so let's se which files contains code for what purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### collectdata.py:- we've used this file to collect data and then store it inside image folder in subfiles individually.\n",
    "\n",
    "[Click here to go directly to collectdata.py](collectdata.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### function.py:- in this file i've included functions which we are going to use later in this project. functions to train the model, to take the output, extract keypoints from out data and form an array so that we can train our model.\n",
    "\n",
    "\n",
    "[Click here to go directly to function.py](function.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### data.py:- this includes code which will extract the keypoints from our data, i.e. the images we've captured and store it in .mp format and store it inside MP_Data folder.\n",
    "\n",
    "\n",
    "[Click here to go directly to data.py](data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### trainmodel.py :- in this i've built a model and trained it with the data i've taken previously.\n",
    "\n",
    "\n",
    "[Click here to go directly to trainmodel.py](trainmodel.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### app.py:- the output that we need from our model is shown here.\n",
    "\n",
    "\n",
    "[Click here to go directly to app.py](app.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
